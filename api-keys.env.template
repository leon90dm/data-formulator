# -----------------------------------------------------------------------------
# OpenAI / OpenAI-Compatible Models
# -----------------------------------------------------------------------------
# Set OPENAI_ENABLED to true to enable OpenAI or any OpenAI-compatible provider.
OPENAI_ENABLED=true

# Your OpenAI API key.
# For custom OpenAI-compatible APIs, this might be optional or a different key.
OPENAI_API_KEY=#your-openai-api-key

# Comma-separated list of OpenAI model names you want to use (e.g., gpt-4.5-preview, o3-mini).
# For custom OpenAI-compatible APIs, list the model names available at your custom OPENAI_API_BASE.
OPENAI_MODELS=gpt-4.5-preview,o3-mini,gpt-4o-mini,o1,o1-mini

# (Optional) Use this if you are using a custom OpenAI-compatible API endpoint
# or a proxy for OpenAI. This should be the base URL of the API service.
# Example for a custom API: OPENAI_API_BASE="https://api.customllmprovider.com/v1"
# Example for Azure OpenAI (though Azure has its own section below, this is also an option if it behaves like OpenAI):
# OPENAI_API_BASE="https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT_NAME"
OPENAI_API_BASE=""

# (Optional) API version, if required by your OpenAI-compatible provider or Azure.
# OPENAI_API_VERSION=""

# Azure OpenAI Configuration
AZURE_ENABLED=true
AZURE_API_KEY=#your-azure-openai-api-key
AZURE_API_BASE=https://your-azure-openai-endpoint.openai.azure.com/
AZURE_API_VERSION=2024-02-15-preview
AZURE_MODELS=o3-mini,gpt-4o

# Anthropic Configuration
ANTHROPIC_ENABLED=true
ANTHROPIC_API_KEY=#your-anthropic-api-key
ANTHROPIC_MODELS=claude-3-7-sonnet-latest,claude-3-5-haiku-latest

# Ollama Configuration
OLLAMA_ENABLED=true
OLLAMA_API_BASE=http://localhost:11434
OLLAMA_MODELS=codellama:7b # models with good code generation capabilities recommended

# if you want to add other models, you can add them with PROVIDER_API_KEY=your-api-key, PROVIDER_MODELS=model1,model2 etc 
# (replacing PROVIDER with the provider name like GEMINI, ANTHROPIC, AZURE, OPENAI, OLLAMA etc. as long as they are supported by LiteLLM)